{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "KyoyuiXRD-wu",
      "metadata": {
        "id": "KyoyuiXRD-wu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "874f9d26",
      "metadata": {
        "id": "874f9d26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "from typing import Tuple, Dict\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "EPSILON = 1e-8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-KR_FWX4Dxqe",
      "metadata": {
        "id": "-KR_FWX4Dxqe"
      },
      "source": [
        "## Q1 - *Forwards*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1318b982",
      "metadata": {
        "id": "1318b982"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(layer_dims : np.ndarray) -> dict:\n",
        "    parameters = {}     # This will hold the parameters\n",
        "\n",
        "    # randomize the weights in each layer and set to zero all the biases\n",
        "    for i in range (1,layer_dims.size):\n",
        "        parameters[f\"W{i}\"] = np.array((np.random.randn(layer_dims[i],layer_dims[i-1])))*np.sqrt(2/layer_dims[i-1])\n",
        "        parameters[f\"b{i}\"] = np.zeros((layer_dims[i],1))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8fa6bdc4",
      "metadata": {
        "id": "8fa6bdc4"
      },
      "outputs": [],
      "source": [
        "def linear_forward(A : np.ndarray, W : np.ndarray, b : np.ndarray)->Tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    This function gets as input activation vector A, weight matrix W and bias vector b for each layer\n",
        "    The output will be vector Z and a dictionary that saves the inpt parameters\n",
        "    \"\"\"\n",
        "    Z = np.dot(W,A) + b\n",
        "\n",
        "    linear_cach = {\"A\" : A,\n",
        "                 \"W\" : W,\n",
        "                 \"b\" : b\n",
        "                }\n",
        "    return Z, linear_cach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f6a1c4b7",
      "metadata": {
        "id": "f6a1c4b7"
      },
      "outputs": [],
      "source": [
        "def Softmax(Z : np.ndarray)->Tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    This function gets as an input the Z vector\n",
        "    The output will be the activation vector for this function using the softmax function and the Z input\n",
        "    \"\"\"\n",
        "    # Z should be np.array\n",
        "    Z_max = np.max(Z, axis=0, keepdims=True)\n",
        "    exp_Z = np.exp(Z - Z_max)\n",
        "    exp_Z_sum = np.sum(exp_Z, axis=0, keepdims=True) + EPSILON\n",
        "    A = exp_Z / exp_Z_sum\n",
        "    activation_cache = {\"Z\": Z}\n",
        "    return A, activation_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2718b874",
      "metadata": {
        "id": "2718b874"
      },
      "outputs": [],
      "source": [
        "def ReLu(Z : np.ndarray)->Tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    This function gets as an input the Z vector\n",
        "    The output will be the activation vector for this function using the ReLu function and the Z input\n",
        "    \"\"\"\n",
        "\n",
        "    relu_func = lambda Z : np.maximum(0,Z)\n",
        "    A = relu_func(Z)\n",
        "    activation_cache  = Z\n",
        "\n",
        "    return A, activation_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "50e48cc5",
      "metadata": {
        "id": "50e48cc5"
      },
      "outputs": [],
      "source": [
        "def linear_activation_forward(A_prev : np.ndarray, W : np.ndarray, b : np.ndarray, activation : str) -> Tuple[np.ndarray , dict]:\n",
        "    \"\"\"\n",
        "    This function inputs are the previous layer activation, its weight matrix and the activation function\n",
        "    The output is the activation vector and activation cach and the dictionary saving the information\n",
        "    \"\"\"\n",
        "    Z, linear_cach = linear_forward(A_prev, W, b)\n",
        "\n",
        "    if activation == \"softmax\":\n",
        "        A, activation_cache = Softmax(Z)\n",
        "    elif activation == \"relu\":\n",
        "        A, activation_cache = ReLu(Z)\n",
        "    else:\n",
        "        raise ValueError(\"The Activation code is not recognizable\")\n",
        "\n",
        "    dict_update = ({\"Z\" : activation_cache})\n",
        "    cach = {**linear_cach,**dict_update}\n",
        "\n",
        "    return A, cach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c4bdff93",
      "metadata": {
        "id": "c4bdff93"
      },
      "outputs": [],
      "source": [
        "def L_model_forward(X : np.ndarray, parameters : dict, use_batchnorm : bool) -> Tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Function input:\n",
        "    X - the data, numpy array of shape (input size, number of examples)\n",
        "    parameters – the initialized W and b parameters of each layer - a dictionary with W_i and b_i as titles\n",
        "    use_batchnorm - a boolean flag used to determine whether to apply batchnorm after the activation\n",
        "\n",
        "    Funciton output:\n",
        "    AL – the last post-activation value\n",
        "    caches – a list of all the cache objects generated by the linear_forward function\n",
        "    \"\"\"\n",
        "    # X = X.T\n",
        "    # For the case the number of examples is one\n",
        "    if(X.ndim == 1):\n",
        "        X = X.reshape(-1,1)\n",
        "\n",
        "    num_param = np.shape(X)[0] #number of lines\n",
        "    num_examples = np.shape(X)[1] #number of columns\n",
        "    caches = {}\n",
        "    L = len(parameters) // 2  #number of layers\n",
        "    A_prev = X\n",
        "    for j in range(1,L):\n",
        "\n",
        "        W = parameters[f'W{j}']\n",
        "        b = parameters[f'b{j}']\n",
        "        A_prev, cach = linear_activation_forward(A_prev, W, b,'relu')\n",
        "        caches.update({f'Layer_{j}' : cach})\n",
        "\n",
        "    W = parameters[f'W{L}']\n",
        "    b = parameters[f'b{L}']\n",
        "    # bias = np.array([b]*num_examples).T #creates a bias matrix\n",
        "    AL, cach = linear_activation_forward(A_prev, W, b,'softmax')\n",
        "    caches.update({f'Activation Layer' : cach})\n",
        "\n",
        "    # print (f\"Activation for last layer is {AL}\")\n",
        "\n",
        "    return AL, caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "255d57eb",
      "metadata": {
        "id": "255d57eb"
      },
      "outputs": [],
      "source": [
        "def compute_cost(AL : np.ndarray,Y : np.ndarray)-> int:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    AL – probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)\n",
        "    Y – the labels vector (i.e. the ground truth)\n",
        "\n",
        "    Output:\n",
        "    cost – the cross-entropy cost\n",
        "    \"\"\"\n",
        "    num_examples = np.shape(AL)[1]\n",
        "    cost = -np.sum(np.dot(Y,np.log(AL.T + EPSILON)))\n",
        "    return cost/num_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e8f7086d",
      "metadata": {
        "id": "e8f7086d"
      },
      "outputs": [],
      "source": [
        "def apply_batchnorm(A : np.ndarray)-> int:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    performs batchnorm on the received activation values of a given layer.\n",
        "\n",
        "    Input:\n",
        "    A - the activation values of a given layer\n",
        "\n",
        "    output:\n",
        "    NA - the normalized activation values, based on the formula learned in class\n",
        "    \"\"\"\n",
        "    mu = np.mean(A, axis=0)\n",
        "    var = np.var(A, axis=0)\n",
        "    A_normalized = (A - mu) / np.sqrt(var + EPSILON)\n",
        "\n",
        "    return A_normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fb78c8a",
      "metadata": {
        "id": "0fb78c8a"
      },
      "source": [
        "## Q2 - *Backwards*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9ccf6e56",
      "metadata": {
        "id": "9ccf6e56"
      },
      "outputs": [],
      "source": [
        "def linear_backward(dZ : np.ndarray, cache : dict) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    description:\n",
        "    Implements the linear part of the backward propagation process for a single layer\n",
        "\n",
        "    Input:\n",
        "    dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)\n",
        "    cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Output:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache['A'], cache['W'], cache['b']\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    return dA_prev, dW, db\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f136d751",
      "metadata": {
        "id": "f136d751"
      },
      "outputs": [],
      "source": [
        "def relu_backward(dA : np.ndarray, activation_cache : np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    Implements backward propagation for a ReLU unit\n",
        "\n",
        "    Input:\n",
        "    dA – the post-activation gradient\n",
        "    activation_cache – contains Z (stored during the forward propagation)\n",
        "\n",
        "    Output:\n",
        "    dZ – gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    Z = activation_cache\n",
        "    dZ = np.array(dA, copy=True)  # Converting dA to a correct object\n",
        "\n",
        "    # When Z <= 0, set dZ to 0\n",
        "    dZ[Z <= 0] = 0\n",
        "\n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "0c4c6715",
      "metadata": {
        "id": "0c4c6715"
      },
      "outputs": [],
      "source": [
        "def softmax_backward(dA : np.ndarray, activation_cache : dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    Implements backward propagation for a softmax unit\n",
        "\n",
        "    Input:\n",
        "    dA – the post-activation gradient\n",
        "    activation_cache – contains Z (stored during the forward propagation)\n",
        "\n",
        "    Output:\n",
        "    dZ – gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "\n",
        "    Z = activation_cache['Z']\n",
        "\n",
        "    # Calculate softmax(Z)\n",
        "    # Z_max = np.max(Z, axis=0, keepdims=True)\n",
        "    # exp_Z = np.exp(Z - Z_max)\n",
        "    # sum_expZ = np.sum(exp_Z, axis=0, keepdims=True) + EPSILON\n",
        "    p,_ = Softmax(Z)\n",
        "    # Calculate dZ=p-y (dA=y when using cross-entropy)\n",
        "    dZ = p-dA\n",
        "\n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "9077c2f4",
      "metadata": {
        "id": "9077c2f4"
      },
      "outputs": [],
      "source": [
        "def linear_activation_backward(dA : np.ndarray, cache : dict, activation : str) -> Tuple[np.ndarray,np.ndarray,np.ndarray] :\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.\n",
        "\n",
        "    Some comments:\n",
        "        The derivative of ReLU is f^' (x)={■(1&if x>0@0&otherwise)┤\n",
        "        The derivative of the softmax function is: p_i-y_i, where p_i is the softmax-adjusted probability of the class and y_i is the “ground truth” (i.e. 1 for the real class, 0 for all others)\n",
        "        You should use the activations cache created earlier for the calculation of the activation derivative and the linear cache should be fed to the linear_backward function\n",
        "\n",
        "    Input:\n",
        "    dA – post activation gradient of the current layer\n",
        "    cache – contains both the linear cache and the activations cache\n",
        "\n",
        "    Output:\n",
        "    dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW – Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db – Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache = {}\n",
        "    linear_cache['A'] = cache['A']\n",
        "    linear_cache['W'] = cache['W']\n",
        "    linear_cache['b'] = cache['b']\n",
        "    activation_cache = cache['Z']\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "    elif activation == \"softmax\":\n",
        "        dZ = softmax_backward(dA, activation_cache)\n",
        "    else:\n",
        "        raise ValueError(\"The Activation code is not recognizable\")\n",
        "\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    return dA_prev, dW, db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c0a4c2a8",
      "metadata": {
        "id": "c0a4c2a8"
      },
      "outputs": [],
      "source": [
        "def L_model_backward(AL : np.ndarray, Y : np.ndarray, caches : dict) -> dict:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    Implement the backward propagation process for the entire network.\n",
        "\n",
        "    Some comments:\n",
        "    the backpropagation for the softmax function should be done only once as only the output layers uses it and the RELU should be done iteratively over all the remaining layers of the network.\n",
        "\n",
        "    Input:\n",
        "    AL - the probabilities vector, the output of the forward propagation (L_model_forward)\n",
        "    Y - the true labels vector (the \"ground truth\" - true classifications)\n",
        "    Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache\n",
        "\n",
        "    Output:\n",
        "    Grads - a dictionary with the gradients\n",
        "                grads[\"dA\" + str(l)] = ...\n",
        "                grads[\"dW\" + str(l)] = ...\n",
        "                grads[\"db\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches)  # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    # Y = np.tile(Y,(m,1)).T  # after this line, Y is the same shape as AL\n",
        "\n",
        "    # Initializing the backpropagation\n",
        "    # Gradient of cost with respect to AL\n",
        "    dAL = Y\n",
        "\n",
        "    # Lth layer (softmax -> linear) gradients. Inputs: \"AL, Y, caches\".\n",
        "    current_cache = caches[str(list(caches.keys())[-1])]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"softmax\")\n",
        "\n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (ReLU -> linear) gradients.\n",
        "        current_cache = caches[(list(caches.keys())[l])]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d75ba261",
      "metadata": {
        "id": "d75ba261"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters : dict, grads : dict, learning_rate : int) -> dict:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    Updates parameters using gradient descent\n",
        "\n",
        "    Input:\n",
        "    parameters – a python dictionary containing the DNN architecture’s parameters\n",
        "    grads – a python dictionary containing the gradients (generated by L_model_backward)\n",
        "    learning_rate – the learning rate used to update the parameters (the “alpha”)\n",
        "\n",
        "    Output:\n",
        "    parameters – the updated values of the parameters object provided as input\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2  # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UZ9AHRN7VoPV",
      "metadata": {
        "id": "UZ9AHRN7VoPV"
      },
      "source": [
        "## Q3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KE7mLwyNVrP9",
      "metadata": {
        "id": "KE7mLwyNVrP9"
      },
      "source": [
        "Some comments for the L_layer_model function:\n",
        "\n",
        "1) There should be done some modifications according to Question 4. The data should be divided into training and validation (80-20). The training should be used for the parameters update via Gradient descent and the validation should determine the stopping criterion.\n",
        "\n",
        "2) Every 100 iterations, the training loss (for the whole training set and not only for batch) and the validation loss should be saved.\n",
        "\n",
        "3) No need of iteration_num variable.\n",
        "\n",
        "3) Iteration need to start from 0 and not from 1.\n",
        "\n",
        "4) Put the batch_norm flag as input (flag that indicates whether or not to use\n",
        "   batcnorm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "068c1b72",
      "metadata": {
        "id": "068c1b72"
      },
      "outputs": [],
      "source": [
        "def L_layer_model(X: np.ndarray, Y: np.ndarray, layers_dims: list, learning_rate: float, batch_size: int) -> Tuple[dict, list, list, int]:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    Implements a L-layer neural network. All layers but the last should have the ReLU activation function,\n",
        "    and the final layer will apply the softmax activation function. The size of the output layer should be\n",
        "    equal to the number of labels in the data. Please select a batch size that enables your code to run well\n",
        "    (i.e., no memory overflows while still running relatively fast).\n",
        "\n",
        "    Hint: the function should use the earlier functions in the following order: initialize -> L_model_forward -> compute_cost -> L_model_backward -> update_parameters\n",
        "\n",
        "    Inputs:\n",
        "    X – the input data, a numpy array of shape (height*width , number_of_examples)\n",
        "    Comment: since the input is in grayscale we only have height and width, otherwise it would have been height*width*3\n",
        "    Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)\n",
        "    Layer_dims – a list containing the dimensions of each layer, including the input\n",
        "    batch_size – the number of examples in a single training batch.\n",
        "\n",
        "    Outputs:\n",
        "    parameters – the parameters learnt by the system during the training\n",
        "    (the same parameters that were updated in the update_parameters function).\n",
        "    costs – the values of the costs for training and validation (calculated by the compute_cost function).\n",
        "    One value is to be saved after each 100 training iterations (e.g., 3000 iterations -> 30 values).\n",
        "    iterations - number of iterations (iteration = one backward-forward pass of a batch) used until convergence\n",
        "    \"\"\"\n",
        "\n",
        "    # Dividing the data into training (80%) and validation (20%)\n",
        "    L_examples = X.shape[1]\n",
        "    L_train = int(0.8 * L_examples)\n",
        "    X_train = X[:, 0:L_train]\n",
        "    Y_train = Y[:, 0:L_train]\n",
        "\n",
        "    X_validation = X[:, L_train:]\n",
        "    Y_validation = Y[:, L_train:]\n",
        "\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "    cost_array_train = []  # The training cost\n",
        "    cost_array_validation = []  # The validation cost\n",
        "    iterations = 0\n",
        "    tool = 1E-4\n",
        "    error_validation = 1E4\n",
        "    ind_batch = int(0)\n",
        "\n",
        "    while error_validation > tool:\n",
        "        X_mini_batch = X_train[:, ind_batch: ind_batch + batch_size]\n",
        "        Y_mini_batch = Y_train[:, ind_batch: ind_batch + batch_size]\n",
        "        AL, caches = L_model_forward(X_mini_batch, parameters, False)\n",
        "        grads = L_model_backward(AL, Y_mini_batch, caches)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        if iterations % 100 == 0:\n",
        "            AL_train, _ = L_model_forward(X_train, parameters, False)\n",
        "            AL_validation, _ = L_model_forward(X_validation, parameters, False)\n",
        "            cost_array_train.append(compute_cost(AL_train, Y_train))\n",
        "            cost_array_validation.append(compute_cost(AL_validation, Y_validation))\n",
        "            if iterations > 0:\n",
        "                error_validation = cost_array_validation[-2] - cost_array_validation[-1]\n",
        "\n",
        "        iterations += 1\n",
        "\n",
        "        if ind_batch >= L_train - batch_size:\n",
        "            ind_batch = 0\n",
        "        else:\n",
        "            ind_batch += batch_size\n",
        "\n",
        "    return parameters, cost_array_train, cost_array_validation, iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "09155c99",
      "metadata": {
        "id": "09155c99"
      },
      "outputs": [],
      "source": [
        "def Predict(X : np.ndarray, Y : np.ndarray, parameters : dict) -> Tuple[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    The function receives an input data and the true labels and calculates the accuracy of the trained neural network on the data.\n",
        "\n",
        "    Input:\n",
        "    X – the input data, a numpy array of shape (height*width, number_of_examples)\n",
        "    Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)\n",
        "    Parameters – a python dictionary containing the DNN architecture’s parameters\n",
        "\n",
        "    Output:\n",
        "    accuracy – the accuracy measure of the neural net on the provided data (i.e. the percentage of the samples\n",
        "    for which the correct label receives the hughest confidence score). Use the softmax function to normalize the output values.\n",
        "    \"\"\"\n",
        "\n",
        "    AL, _ = L_model_forward(X, parameters, False)\n",
        "\n",
        "    max_prob_index = np.argmax(AL, axis=0) #This will hold, for each example, the index of the feature with largest probabilty\n",
        "    max_true_index = np.argmax(Y, axis=0) #This will hold, for each example, the index of the feature with largest truth\n",
        "\n",
        "    num_correct_pred = np.sum(max_prob_index == max_true_index) #How many crrect prediction did the network made with this prameters\n",
        "    num_examples = Y.shape[1]\n",
        "\n",
        "    accuracy = num_correct_pred/num_examples\n",
        "\n",
        "    return accuracy, AL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h1dRX7PjfiKH",
      "metadata": {
        "id": "h1dRX7PjfiKH"
      },
      "source": [
        "## Q4 - MNIST Classification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_UKFZkYKl0uQ",
      "metadata": {
        "id": "_UKFZkYKl0uQ"
      },
      "source": [
        "**Generating the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "hNCyDxBmg4Kh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNCyDxBmg4Kh",
        "outputId": "babc916a-60cb-4ef4-cd14-40fcbcfb63df"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Call the MNIST\n",
        "\"\"\"\n",
        "import ssl\n",
        "\n",
        "# Disable SSL certificate verification\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "mnist = fetch_openml('mnist_784')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bBndU0aol8Fb",
      "metadata": {
        "id": "bBndU0aol8Fb"
      },
      "source": [
        "**Start the classification process:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "MKdRzubzfpg5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKdRzubzfpg5",
        "outputId": "1834c2a5-d787-4907-8c92-c9be8c9ec3ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py:528: RuntimeWarning: invalid value encountered in cast\n",
            "  fill_value = lib.item_from_zerodim(np.array(np.nan).astype(dtype))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X: (784, 70000)\n",
            "Shape of Y: (10, 70000)\n"
          ]
        }
      ],
      "source": [
        "# np.random.seed(1)\n",
        "X, Y_non_binary = mnist[\"data\"], mnist[\"target\"]\n",
        "# Convert target to integers\n",
        "Y_non_binary = Y_non_binary.astype(int)\n",
        "# Convert target to One-Hot Encoding:\n",
        "num_classes = 10\n",
        "Y = np.zeros((Y_non_binary.shape[0], num_classes))\n",
        "Y[np.arange(Y_non_binary.shape[0]), Y_non_binary] = 1\n",
        "\n",
        "# Example of using the data\n",
        "\n",
        "# Convert to numpy arrays if not already\n",
        "X = np.array(X).T/255\n",
        "Y = np.array(Y).T\n",
        "print(\"Shape of X:\", X.shape)  # Should output (784, 70000)\n",
        "print(\"Shape of Y:\", Y.shape)  # Should output (10, 70000)\n",
        "\n",
        "# Dividing the data into training (80%) and testing (20%). The training-testing ratio can be changed\n",
        "L_examples = X.shape[1]\n",
        "L_train = int(0.8 * L_examples)\n",
        "X_train = X[:, :L_train]\n",
        "Y_train = Y[:, :L_train]\n",
        "X_test = X[:, L_train:]\n",
        "Y_test = Y[:, L_train:]\n",
        "\n",
        "# Network parameters\n",
        "layer_dimension = np.array([X.shape[0], 20, 7, 5, 10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "aec2c4aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters, cost_array_train, cost_array_validation, iterations =  L_layer_model(X_train,Y_train,layer_dimension,float(9e-3),int(200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "5d8587eb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 2)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(cost_array_train), int(iterations/100)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "3c57ae93",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x21401182370>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "it = np.arange(0,int(iterations/100)+1)\n",
        "\n",
        "plt.plot(it*100,cost_array_train, color = 'black')\n",
        "plt.scatter(it*100,cost_array_validation, color = 'blue')\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "f1deea68",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sum of AL is [1.         0.99999999 1.         ... 1.         1.         1.        ]\n"
          ]
        }
      ],
      "source": [
        "acc,a = Predict(X_train,Y_train,parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "ec05071f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.21869642857142857"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "8a8a20af",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.237975</td>\n",
              "      <td>0.008167</td>\n",
              "      <td>-0.372836</td>\n",
              "      <td>0.196120</td>\n",
              "      <td>-0.183504</td>\n",
              "      <td>0.434111</td>\n",
              "      <td>-0.488822</td>\n",
              "      <td>0.452078</td>\n",
              "      <td>-0.560234</td>\n",
              "      <td>0.163456</td>\n",
              "      <td>-0.371660</td>\n",
              "      <td>-0.092976</td>\n",
              "      <td>-0.321391</td>\n",
              "      <td>0.219968</td>\n",
              "      <td>-0.267495</td>\n",
              "      <td>-0.271414</td>\n",
              "      <td>-0.074937</td>\n",
              "      <td>-0.290646</td>\n",
              "      <td>0.329694</td>\n",
              "      <td>-0.213763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.148441</td>\n",
              "      <td>0.630465</td>\n",
              "      <td>-0.135511</td>\n",
              "      <td>-0.077286</td>\n",
              "      <td>0.212930</td>\n",
              "      <td>-0.607484</td>\n",
              "      <td>0.702503</td>\n",
              "      <td>0.800684</td>\n",
              "      <td>0.062931</td>\n",
              "      <td>1.017036</td>\n",
              "      <td>0.387758</td>\n",
              "      <td>0.108400</td>\n",
              "      <td>-0.226085</td>\n",
              "      <td>-0.529272</td>\n",
              "      <td>0.019381</td>\n",
              "      <td>-0.536065</td>\n",
              "      <td>-0.037735</td>\n",
              "      <td>0.361596</td>\n",
              "      <td>-0.116261</td>\n",
              "      <td>0.338122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.127389</td>\n",
              "      <td>0.172524</td>\n",
              "      <td>0.390304</td>\n",
              "      <td>0.282888</td>\n",
              "      <td>0.732124</td>\n",
              "      <td>0.577476</td>\n",
              "      <td>0.160080</td>\n",
              "      <td>0.430464</td>\n",
              "      <td>-0.204069</td>\n",
              "      <td>-0.218422</td>\n",
              "      <td>0.088829</td>\n",
              "      <td>-0.737188</td>\n",
              "      <td>-0.363900</td>\n",
              "      <td>-0.280313</td>\n",
              "      <td>-0.030606</td>\n",
              "      <td>-0.266303</td>\n",
              "      <td>0.227253</td>\n",
              "      <td>-0.204013</td>\n",
              "      <td>-0.131200</td>\n",
              "      <td>0.025853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.219899</td>\n",
              "      <td>-0.124378</td>\n",
              "      <td>-0.048462</td>\n",
              "      <td>0.172059</td>\n",
              "      <td>0.459846</td>\n",
              "      <td>0.566336</td>\n",
              "      <td>-0.213754</td>\n",
              "      <td>-0.663842</td>\n",
              "      <td>-0.096213</td>\n",
              "      <td>0.147313</td>\n",
              "      <td>0.362739</td>\n",
              "      <td>0.412665</td>\n",
              "      <td>0.463429</td>\n",
              "      <td>0.617387</td>\n",
              "      <td>0.459023</td>\n",
              "      <td>-0.210563</td>\n",
              "      <td>0.292393</td>\n",
              "      <td>0.502796</td>\n",
              "      <td>0.264828</td>\n",
              "      <td>-0.648630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.023138</td>\n",
              "      <td>-0.459974</td>\n",
              "      <td>-0.094852</td>\n",
              "      <td>0.577314</td>\n",
              "      <td>0.382130</td>\n",
              "      <td>-0.240936</td>\n",
              "      <td>0.544528</td>\n",
              "      <td>-0.241391</td>\n",
              "      <td>-0.129671</td>\n",
              "      <td>0.394482</td>\n",
              "      <td>0.529560</td>\n",
              "      <td>-0.669959</td>\n",
              "      <td>0.147032</td>\n",
              "      <td>0.236594</td>\n",
              "      <td>0.167888</td>\n",
              "      <td>-0.158655</td>\n",
              "      <td>-0.472602</td>\n",
              "      <td>0.392581</td>\n",
              "      <td>0.721358</td>\n",
              "      <td>0.459673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.118718</td>\n",
              "      <td>0.022657</td>\n",
              "      <td>-0.054367</td>\n",
              "      <td>0.124249</td>\n",
              "      <td>0.232119</td>\n",
              "      <td>-0.564402</td>\n",
              "      <td>-0.312319</td>\n",
              "      <td>-0.072657</td>\n",
              "      <td>-0.236111</td>\n",
              "      <td>0.641208</td>\n",
              "      <td>-0.101095</td>\n",
              "      <td>0.319638</td>\n",
              "      <td>-0.425976</td>\n",
              "      <td>-0.051619</td>\n",
              "      <td>-0.345343</td>\n",
              "      <td>0.053389</td>\n",
              "      <td>0.287743</td>\n",
              "      <td>-0.335347</td>\n",
              "      <td>-0.011797</td>\n",
              "      <td>-0.265535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.044648</td>\n",
              "      <td>0.088195</td>\n",
              "      <td>-0.045447</td>\n",
              "      <td>-0.008388</td>\n",
              "      <td>0.282878</td>\n",
              "      <td>-0.306269</td>\n",
              "      <td>-0.577847</td>\n",
              "      <td>-0.029311</td>\n",
              "      <td>0.121954</td>\n",
              "      <td>0.009567</td>\n",
              "      <td>0.185532</td>\n",
              "      <td>-0.040022</td>\n",
              "      <td>-0.174968</td>\n",
              "      <td>-0.557409</td>\n",
              "      <td>0.158902</td>\n",
              "      <td>-0.033311</td>\n",
              "      <td>0.037673</td>\n",
              "      <td>-0.256681</td>\n",
              "      <td>0.892976</td>\n",
              "      <td>0.225374</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0  0.237975  0.008167 -0.372836  0.196120 -0.183504  0.434111 -0.488822   \n",
              "1 -0.148441  0.630465 -0.135511 -0.077286  0.212930 -0.607484  0.702503   \n",
              "2 -0.127389  0.172524  0.390304  0.282888  0.732124  0.577476  0.160080   \n",
              "3  0.219899 -0.124378 -0.048462  0.172059  0.459846  0.566336 -0.213754   \n",
              "4  0.023138 -0.459974 -0.094852  0.577314  0.382130 -0.240936  0.544528   \n",
              "5 -0.118718  0.022657 -0.054367  0.124249  0.232119 -0.564402 -0.312319   \n",
              "6 -0.044648  0.088195 -0.045447 -0.008388  0.282878 -0.306269 -0.577847   \n",
              "\n",
              "         7         8         9         10        11        12        13  \\\n",
              "0  0.452078 -0.560234  0.163456 -0.371660 -0.092976 -0.321391  0.219968   \n",
              "1  0.800684  0.062931  1.017036  0.387758  0.108400 -0.226085 -0.529272   \n",
              "2  0.430464 -0.204069 -0.218422  0.088829 -0.737188 -0.363900 -0.280313   \n",
              "3 -0.663842 -0.096213  0.147313  0.362739  0.412665  0.463429  0.617387   \n",
              "4 -0.241391 -0.129671  0.394482  0.529560 -0.669959  0.147032  0.236594   \n",
              "5 -0.072657 -0.236111  0.641208 -0.101095  0.319638 -0.425976 -0.051619   \n",
              "6 -0.029311  0.121954  0.009567  0.185532 -0.040022 -0.174968 -0.557409   \n",
              "\n",
              "         14        15        16        17        18        19  \n",
              "0 -0.267495 -0.271414 -0.074937 -0.290646  0.329694 -0.213763  \n",
              "1  0.019381 -0.536065 -0.037735  0.361596 -0.116261  0.338122  \n",
              "2 -0.030606 -0.266303  0.227253 -0.204013 -0.131200  0.025853  \n",
              "3  0.459023 -0.210563  0.292393  0.502796  0.264828 -0.648630  \n",
              "4  0.167888 -0.158655 -0.472602  0.392581  0.721358  0.459673  \n",
              "5 -0.345343  0.053389  0.287743 -0.335347 -0.011797 -0.265535  \n",
              "6  0.158902 -0.033311  0.037673 -0.256681  0.892976  0.225374  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(parameters['W2'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
